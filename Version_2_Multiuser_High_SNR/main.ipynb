{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c46817c8-5917-4a94-a77a-1c6c589a626b","_uuid":"4c968fd4-baba-4513-ab2a-d9535925f7e0","id":"QCHK-aChxg3G","trusted":true},"source":["# DRL Approach for Optimal Precoder Design in MU-MISO: High SNR & Equal Power Constraint\n","\n","## Overview\n","This project explores a Deep Reinforcement Learning (DRL) approach for designing optimal precoder vectors in a Multi-User MISO (MU-MISO) system, specifically under **High Signal-to-Noise Ratio (SNR)** conditions with a constraint of equal power allocation for each user. The aim is to maximize the sum rate, addressing high-SNR scenarios where signal quality is generally strong. The constraint that each user has unit power further challenges the decision-making process, requiring precise tuning for optimal performance.\n","\n","## General High SNR\n","This version focuses on high-SNR conditions, where the signal quality is robust and noise has a minimal impact. Training under these conditions allows the DRL agent to leverage high-quality channels to achieve efficient precoder design within the equal power constraint.\n","\n","### Objectives\n","- **Maximized Efficiency in High SNR:** Enable the model to capitalize on high-SNR conditions for efficient precoding strategies that optimize sum rate.\n","- **Equal Power Allocation Constraint:** Enforce a strict equal power allocation constraint, ensuring that each user’s power remains at a unit level, thereby adding an extra layer of complexity.\n","- **Comparison with Traditional Methods:** Benchmark the DRL-based approach against traditional linear precoding techniques such as MRT, ZF, and MMSE to highlight the advantages of DRL in constrained high-SNR environments.\n","\n","### Implementation Details\n","- **Input Configuration:** The model architecture integrates SNR and channel information, enhancing the model’s ability to dynamically adapt in high-SNR environments.\n","- **Equal Power Constraint:** The DRL agent is trained to maintain equal power allocation across users, optimizing for sum rate within these bounds.\n","- **Training Process:** Training is conducted in high-SNR environments to allow the agent to develop robust strategies, focusing on performance maximization within the constraint.\n","- **Results Analysis:** Compare the DRL-based model’s outcomes with traditional techniques, evaluating the sum rate and overall efficiency of the system in high-SNR conditions."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import Packeges\n","import numpy as np\n","import tensorflow as tf\n","import os\n","\n","from DDPG_MultiUser import Agent\n","from utils import plot_learning_curve"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"231aa792-d754-4127-8747-67c1e2c53185","_uuid":"ab7196a9-c728-4d8f-a583-db8621e78c35","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:00:08.401581Z","iopub.status.busy":"2023-07-11T18:00:08.401260Z","iopub.status.idle":"2023-07-11T18:00:08.438806Z","shell.execute_reply":"2023-07-11T18:00:08.438034Z","shell.execute_reply.started":"2023-07-11T18:00:08.401555Z"},"id":"r1HskVuwriq4","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# We use this to build the first run based on the results of the previous run.\n","changing_alg = 'logReward_constrain_highSNR'\n","prev_run_num ='3'\n","run_num = '4'\n","figure_file = 'plots/model_'+changing_alg+'/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["snr_user = 10   ##   Standard deviation of channel noice\n","n_tx = 6   # number of atennas\n","n_users = 4 # number of users\n","# This is the importance of each user  \n","weight_rate = np.full((n_users,),0.25)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a191aafe-2ad5-4f3f-8590-4f4552dd6ed0","_uuid":"3eca2d51-b369-45c3-b336-1d434be63e19","id":"bJPKeJFivoZR","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Helpful functions"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def turn_mat_to_complex(matrix,n_tx = 6):\n","    \"\"\"Turn real matrix to complex matrix.\"\"\"\n","    if matrix.shape[0] ==1:\n","        matrix = np.reshape(matrix,(-1,2*n_tx))\n","    return matrix[:,:n_tx]+ 1j*matrix[:,n_tx:]\n","\n","def PAE(states,n_tx = 6):\n","    \"\"\"Phase ambiguity elimination to reach a unique solution.\"\"\"\n","    h_ = turn_mat_to_complex(states,n_tx)\n","    h_zero_phase = h_*np.reshape(np.exp(-1j*np.angle(h_))[:,0],(-1,1))\n","    h_pae_row = np.concatenate([np.real(h_zero_phase),np.imag(h_zero_phase)],axis=1).reshape((1,-1))\n","    return h_pae_row"]},{"cell_type":"markdown","metadata":{},"source":["## Channel Model\n","\n","The channel model can be represented as:\n","\n","$$\n","\\mathbf{Y} = \\sqrt{\\rho} \\cdot \\mathbf{H} \\mathbf{W}^H \\mathbf{s} + \\mathbf{n}\n","$$\n","\n","where:\n","\n","- ρ is the input SNR, defined as ρ = Pt / σn².\n","- H is a matrix with dimensions K x n_tx.\n","\n","### Definitions:\n","\n","- K: Number of users.\n","- n<sub>tx</sub>: Number of antennas at the base station (BS)."]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"720efede-13db-4988-b52f-c79b00823002","_uuid":"aacb5dfb-e04b-45cd-a5f2-26a2d7db71ac","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:00:18.126079Z","iopub.status.busy":"2023-07-11T18:00:18.125103Z","iopub.status.idle":"2023-07-11T18:00:18.137908Z","shell.execute_reply":"2023-07-11T18:00:18.136512Z","shell.execute_reply.started":"2023-07-11T18:00:18.126049Z"},"id":"cjcQNZ34voZS","jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["def channel_states(n_users = 4, n_tx = 6, seed_number = 42 , random = False):\n","    \"\"\"Channel model.\"\"\"\n","    # H represents the channel states and has a size of (# of users, # of transmitted antennas).\n","    if not random:\n","        np.random.seed(seed_number)\n","    real =(1/np.sqrt(2))  * np.random.randn(n_users,n_tx).astype(np.float32)  # 4x 6\n","    imag = (1/np.sqrt(2)) * np.random.randn(n_users,n_tx).astype(np.float32) # 4x6\n","    H = np.hstack([real , imag]) #4x 12\n","    H = np.reshape(H,(1,-1)) #1 x 12*4(48)\n","    return H"]},{"cell_type":"markdown","metadata":{},"source":["# The Reward Equation\n","\n","### The reward equation is given by:\n","\n","$$\n","\\text{Reward} = \\sum_{k=1}^{K} (\\nu_k R_k) = \\sum_{k=1}^{K} \\left( \\nu_k \\cdot \\log_2 \\left( 1 + \\frac{(\\rho |h_k \\cdot w_k^H|)^2}{1 + \\rho \\sum_{j=1, j \\neq k}^{K} |h_k \\cdot w_j^H|^2} \\right) \\right)\n","$$\n","\n","### Where:\n","- ρ is the input SNR\n","- K: Number of users.\n","- n<sub>tx</sub>: Number of antennas at the base station (BS).\n","- h: Channel state.\n","- w: Precoder vector."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","def get_reward_vectorize(h_estimate, precoder_mat,n_tx = 6,snr = 1.0):\n","    \"\"\"Get reward as a vector.\"\"\"\n","    # Reward equation sum weighted rate of users.\n","    h_estimate = turn_mat_to_complex(h_estimate,n_tx) # (# of users, # of transmitted antennas)\n","    \n","    precoder_mat = turn_mat_to_complex(precoder_mat,n_tx) # (# of users, # of transmitted antennas)\n","    precoder_mat_H = (precoder_mat).conjugate().T  # (# of transmitted antennas, # of users) due to .T\n","    ## Multiplication of the channel estimate matrix by the precoder vector.\n","    #   - The diagonal elements represent the power without interference, \n","    #     so hi_py_wi[i, i] is the power at user i.\n","    #   - The off-diagonal elements represent the interference, \n","    #     so hi_py_wi[i, j] where i ≠ j represents the interference at user i.\n","    hi_py_wi = snr*np.square(np.abs(np.matmul(h_estimate,precoder_mat_H))) \n","    power = np.diag(hi_py_wi) \n","    interference = hi_py_wi.copy()\n","    # Make diognal zero so sum of each Row is the interference only\n","    np.fill_diagonal(interference,0) \n","\n","    interference = 1 +np.sum(interference,axis=1)\n","    reward_vector =  np.log2(1+power/interference)\n","    return reward_vector"]},{"cell_type":"markdown","metadata":{"_cell_guid":"12fcf2a3-592d-4043-99f2-8c1c1c777ecc","_uuid":"ae63f490-1897-441a-9ba8-7852925ae43b","id":"qGAehqg-Fj34","trusted":true},"source":["# Function for Evaluation\n","This function is highly optimized with vectorization, allowing it to run very quickly.\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def turn_3d_to_C(csi,n_tx = 6):\n","    \"\"\"Convert a 3D real array into a complex array.\"\"\"\n","    # CSI stands for Channel State Information.\n","    #   csi.shape[0] is the number of instances I want to evaluate.\n","    matrix = np.reshape(csi,(csi.shape[0],-1,2*n_tx))\n","    return matrix[:,:,:n_tx]+ 1j*matrix[:,:,n_tx:]\n","\n","def PAE_3d(H,n_tx = 6):\n","    \"\"\"Perform phase ambiguity elimination for a 3D matrix.\"\"\"\n","    H_c = turn_3d_to_C(H,n_tx)\n","    H_zero_phase = H_c*np.exp(-1j*np.expand_dims(np.angle(H_c)[:,:,0],axis=2))\n","    h_pae_row = np.concatenate([np.real(H_zero_phase),np.imag(H_zero_phase)],axis=2)\n","    return h_pae_row"]},{"cell_type":"markdown","metadata":{},"source":["## Generate H Matrix\n","\n","3-D matrix dimensions: (number of instances, number of users, number of transmitted antennas)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def channel_mat_seed(n_runs =5000,n_users = 4, n_tx = 6, lower_seed = 0):\n","    \"\"\"Generate an H matrix where each row is initialized with a different seed.\"\"\"\n","    matrix = np.zeros((n_runs,n_users,n_tx*2),dtype=np.float32)\n","    for i in range(lower_seed,n_runs + lower_seed):\n","        np.random.seed(i)\n","        real =(1/np.sqrt(2))  * np.random.randn(n_users,n_tx).astype(np.float32)\n","        imag = (1/np.sqrt(2)) * np.random.randn(n_users,n_tx).astype(np.float32)\n","        matrix[i-lower_seed] = np.hstack([real , imag])\n","    return matrix\n","\n","def channel_matrix(n_evaluation ,n_users = 4,n_tx= 6, seed_number = 42 ,random = False):\n","    \"\"\"Generate an H matrix where each row is fully random and initialized with a different seed.\"\"\"\n","    if not random:\n","        np.random.seed(seed_number)\n","    real =(1/np.sqrt(2))  * np.random.randn(n_evaluation,n_users,n_tx).astype(np.float32)\n","    imag = (1/np.sqrt(2)) * np.random.randn(n_evaluation,n_users,n_tx).astype(np.float32)\n","    H = np.dstack([real , imag])\n","    # H = np.reshape(H,(n_evaluation,1,-1))\n","    return H"]},{"cell_type":"markdown","metadata":{},"source":["## Function to Calculate Reward for 3D H Matrix\n","\n","This function calculates the reward for a 3D channel states matrix.\n","\n","**Parameters:**\n","\n","- **H**: A 3D channel states matrix with dimensions (number of instances, number of users, number of transmitted antennas).\n","- **W**: A precoder matrix with the same dimensions as H.\n","\n","**Returns:**\n","\n","- A reward matrix with dimensions (number of instances, number of users) for all instances and users."]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"5508d5a1-3b60-46d3-bb2c-b42e96f5c1c7","_uuid":"426758c7-be37-4ff9-ae07-76a44ddb6295","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:02:22.940620Z","iopub.status.busy":"2023-07-11T18:02:22.940190Z","iopub.status.idle":"2023-07-11T18:02:23.192353Z","shell.execute_reply":"2023-07-11T18:02:23.190508Z","shell.execute_reply.started":"2023-07-11T18:02:22.940592Z"},"id":"G2a1oA75zUy6","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def reward_for_3d(H_3d, precoder_3d,snr = 1.0):\n","    \"\"\"Compute the reward function for multiple instances using vectorization to speed up computations.\"\"\"\n","    n_tx = H_3d.shape[2]//2   ## Number of transmitted antennas\n","    \n","    ### Turn 3d matrix to Comlex\n","    h_estimate = turn_3d_to_C(H_3d,n_tx) # Has size (n_runs x n_user x n_tx)\n","    precoder_3d = turn_3d_to_C(precoder_3d,n_tx)\n","    # Transpose along the y and z axes.\n","    precoder_3d_t = np.transpose(precoder_3d.conjugate(),axes=[0 ,2 ,1]) # Has size (n_runs x n_tx x n_user)\n","    ### Get all combination of   |hi*wi|**2\n","    H_py_W = snr*np.square(np.abs(np.matmul(h_estimate,precoder_3d_t))) # Has size (n_runs x n_user x n_user)\n","    \n","    ### Diagonal elements\n","    power = np.diagonal(H_py_W,axis1=1,axis2=2)  # Has size (n_runs x n_user )\n","    ###  Fill diagonal with zeros\n","    interference = H_py_W.copy()\n","    diagonal_index = np.arange(H_3d.shape[1])\n","    interference[:, diagonal_index, diagonal_index] = 0\n","    ### Get the off diagonal elments\n","    interference = 1.0 +np.sum(interference,axis=2)\n","    reward_vector =  np.log2(1+power/interference)\n","    return reward_vector"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Precoder Techniques to Evaluate Our Model\n","\n","1. **MRT**  (Maximum Ratio Transmission)\n","2. **ZF**   (Zero-Forcing)\n","3. **MMSE** (Minimum Mean Square Error)\n","\n","The precoder matrices are defined as follows:\n","\n","$$\n","\\begin{align*}\n","W_{MRT} & = H \\\\\n","W_{ZF} & = H(H^H H)^{-1} \\\\\n","W_{MMSE} & = H(H^H H + \\frac{1}{\\rho} I_K)\n","\\end{align*}\n","$$"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def sum_norm_root_square(H):\n","    \"\"\"Calculate the summation of the squared norm of the channel matrix H.\"\"\"\n","    sum_norm = np.sqrt(np.sum(np.square(np.linalg.norm(H,axis=2)),axis=1))\n","    sum_norm = np.repeat(sum_norm,H.shape[1]).reshape(H.shape[0],H.shape[1])  ### To help us on Broadcast\n","    sum_norm = np.expand_dims(sum_norm,axis=2)       ### has n_runs x n_users x1 dimentions\n","    return sum_norm\n","\n","def MRT(H):  ### n_runs x n_user x n_tx*2\n","    \"\"\"MRT stands for Maximum Ratio Transmission, a linear precoder technique.\"\"\"\n","    precoder = H/np.expand_dims(np.linalg.norm(H,axis=2),axis=2)\n","    return precoder\n","\n","def zf_mat(H):\n","    \"\"\"ZF stands for Zero-Forcing, a linear precoder technique.\"\"\"\n","    n_tx = H.shape[2]//2\n","    H = turn_3d_to_C(H,n_tx=n_tx)\n","    zf_H = np.matmul(np.linalg.inv(np.matmul(H,np.transpose(H.conjugate(),axes=[0,2,1]))),H)\n","    precoder_c  = zf_H/np.expand_dims(np.linalg.norm(zf_H,axis=2),axis=2)\n","    precoder = np.concatenate([np.real(precoder_c),np.imag(precoder_c)],axis=2)\n","    return precoder\n","\n","def mmse_mat(H,snr= 1.0):\n","    \"\"\"MMSE stands for Minimum Mean Square Error, a linear precoder technique.\"\"\"\n","    n_tx = H.shape[2]//2\n","    H = turn_3d_to_C(H,n_tx=n_tx)\n","    identity_3d = np.zeros((H.shape[0],H.shape[1],H.shape[1]),dtype=np.float32)\n","    idx = np.arange(H.shape[1])\n","    identity_3d[:,idx,idx] = 1/snr\n","    mmse_H = np.matmul(np.linalg.inv(np.matmul(H,np.transpose(H.conjugate(),axes=[0,2,1]))+identity_3d),H)\n","    precoder_c  = mmse_H/np.expand_dims(np.linalg.norm(mmse_H,axis=2),axis=2)\n","    precoder = np.concatenate([np.real(precoder_c),np.imag(precoder_c)],axis=2)\n","    return precoder"]},{"cell_type":"markdown","metadata":{},"source":["### Put all together."]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"74343291-7568-4e47-9e05-043e0954f92a","_uuid":"df9a9a2c-1614-4ef9-9348-930c6eb3cfa2","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:02:23.194643Z","iopub.status.busy":"2023-07-11T18:02:23.194269Z","iopub.status.idle":"2023-07-11T18:02:23.211641Z","shell.execute_reply":"2023-07-11T18:02:23.210344Z","shell.execute_reply.started":"2023-07-11T18:02:23.194620Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def evaluate_while_training(state_for_eval, agent, snr_user):\n","    \"\"\"Evaluate the performance of the agent during training compared to other methods.\"\"\"\n","    # Calculate cumulative rewards for the Linear techniques\n","    cumulative_reward_mrt = reward_for_3d(state_for_eval, MRT(state_for_eval), snr=snr_user)\n","    cumulative_reward_zf = reward_for_3d(state_for_eval, zf_mat(state_for_eval), snr=snr_user)\n","    cumulative_reward_mmse = reward_for_3d(state_for_eval, mmse_mat(state_for_eval, snr=snr_user), snr=snr_user)\n","    \n","    # Get the action matrix from the agent based on the evaluated state, reshaped for the input format\n","    action_mat_in = tf.reshape(\n","        agent.choose_action(state_for_eval.reshape((state_for_eval.shape[0], 1, -1)), evaluate=True),\n","        (state_for_eval.shape[0], n_users, -1))\n","    \n","    # Calculate rewards for the actions chosen by the agent using the channel state\n","    reward_mat_in = reward_for_3d(state_for_eval, action_mat_in, snr=snr_user)\n","    \n","    # Average the rewards obtained by the actor (agent) across all users\n","    ave_reward_actor = np.mean(reward_mat_in, axis=0)\n","    \n","    # Average the cumulative rewards for the linear methods\n","    ave_reward_mat_mrt = np.mean(cumulative_reward_mrt, axis=0)\n","    ave_reward_mat_zf = np.mean(cumulative_reward_zf, axis=0)\n","    ave_reward_mat_mmse = np.mean(cumulative_reward_mmse, axis=0)\n","    \n","    # Calculate the percentage of the average reward from the actor compared to the Linear methods\n","    percentage_mrt = np.sum(np.mean(reward_mat_in, axis=0)) * 100.0 / np.sum(ave_reward_mat_mrt)\n","    percentage_zf = np.sum(np.mean(reward_mat_in, axis=0)) * 100.0 / np.sum(ave_reward_mat_zf)\n","    percentage_mmse = np.sum(np.mean(reward_mat_in, axis=0)) * 100.0 / np.sum(ave_reward_mat_mmse)\n","    # Return the average rewards and percentages for MRT, ZF, and MMSE comparisons\n","    return ave_reward_actor, ave_reward_mat_mrt, ave_reward_mat_zf, ave_reward_mat_mmse, percentage_mrt, percentage_zf, percentage_mmse"]},{"cell_type":"markdown","metadata":{"_cell_guid":"73bca18f-1cd2-435d-bc99-33fda545eee6","_uuid":"6e3df2d3-f37e-4d91-819f-34fb069822ed","id":"D0RFplI5voZU","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Agent Initialization\n","\n","This code initializes an instance of the `Agent` class with the following parameters:\n","\n","- **input_dims**: A tuple representing the dimensions of the input, calculated as (2 x number of antennas x number of users)\n","- **alpha**: Learning rate, set to 3.3e-5.\n","- **beta**: Weight decay parameter, set to 6.7e-5.\n","- **gamma**: Discount factor, set to 0.1.\n","- **n_users**: The number of users.\n","- **n_tx**: The number of transmitted antennas.\n","- **max_size**: The maximum size of the replay buffer, set to 1,000,000.\n","- **tau**: Soft update parameter, set to 0.005.\n","- **fc1**, **fc2**, **fc3**: Number of neurons in the first, second, and third fully connected layers, set to 1024, 512, and 128, respectively.\n","- **batch_size**: Size of the mini-batch for training, set to 64.\n","- **noise**: Noise of the channel, set to 3.0e-4.\n","- **changing_alg**: The name of the path to save the model.\n","\n","The agent will use these parameters to interact with the environment and learn an optimal policy.\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["agent = Agent(\n","    input_dims=(2 * n_tx * n_users,),\n","    alpha=2e-05,\n","    beta=4e-05,\n","    gamma=0.05,\n","    n_users=n_users,\n","    n_tx=n_tx,\n","    max_size=1000000,\n","    tau=0.005,\n","    fc1=1024,\n","    fc2=512,\n","    fc3=128,\n","    batch_size=64,\n","    noise=2.0e-4,\n","    changing_alg=changing_alg\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["alpha = agent.alpha \n","beta = agent.beta\n","noise = agent.noise"]},{"cell_type":"markdown","metadata":{"_cell_guid":"43167bc8-ae15-47c9-86de-36a90e398439","_uuid":"b6c46a92-3244-4a11-ab5b-45925cfd586a","id":"Wppie83VvoZV","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["### Loading Previous Weights\n","\n","This section is responsible for loading the previous weights, allowing the agent to resume learning from the results of the previous run. This functionality can help improve the convergence of the learning process by leveraging previously acquired knowledge. -->"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T18:02:27.483466Z","iopub.status.busy":"2023-07-11T18:02:27.483067Z","iopub.status.idle":"2023-07-11T18:02:27.933205Z","shell.execute_reply":"2023-07-11T18:02:27.931666Z","shell.execute_reply.started":"2023-07-11T18:02:27.483438Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["____Loading model of (logReward_constrain_highSNR) version At_end_3____\n"]}],"source":["### uncomment to be suitble in kaggle\n","# path = agent.model_path\n","# agent.model_path = \"../input/last-lsnr/\"+agent.model_path\n","if(os.path.exists(agent.model_path)):\n","    agent.learn(evaluate=True)\n","    agent.load_models(name='At_end_'+prev_run_num)\n","# agent.model_path = path"]},{"cell_type":"markdown","metadata":{"_cell_guid":"09a9eaf4-0d3c-4b1f-a9a0-8e43bdf83895","_uuid":"f28ef598-d47c-4f19-b81f-49c6aa5a6fd7","id":"QZEl3miOvoZW","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Training the agent"]},{"cell_type":"code","execution_count":33,"metadata":{"_cell_guid":"0fb60fdb-cca7-4079-90ac-5bb5ff1e2b6b","_uuid":"bca43750-5335-4420-bbb6-152b9c9d9b04","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T02:21:41.876513Z","iopub.status.busy":"2023-07-11T02:21:41.876144Z","iopub.status.idle":"2023-07-11T02:21:41.890510Z","shell.execute_reply":"2023-07-11T02:21:41.889088Z","shell.execute_reply.started":"2023-07-11T02:21:41.876483Z"},"id":"Ch8j1pPqB7UQ","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["n_episode = 5\n","n_time_steps = 100 "]},{"cell_type":"code","execution_count":35,"metadata":{"_cell_guid":"b0e31da0-f6de-4493-a8cf-90ccf72cf46f","_uuid":"b99e0f53-cf9d-4738-8303-61ce15d0e56c","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T02:21:41.893685Z","iopub.status.busy":"2023-07-11T02:21:41.893259Z","iopub.status.idle":"2023-07-11T02:22:11.095429Z","shell.execute_reply":"2023-07-11T02:22:11.093444Z","shell.execute_reply.started":"2023-07-11T02:21:41.893650Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The percentage MRT reward = 90.52336983016748\n","The percentage ZF reward = 169.64402250276402\n","The percentage mmse reward = 90.3287748818272\n","____Saving model of (logReward_constrain_lowSNR) version safe_3____\n","\n","\n","____Saving model of (logReward_constrain_lowSNR) version best_3____\n","episode 0 score MRT 90.5 score ZF 169.6 avg percentage 90.5\n","The percentage MRT reward = 90.84690414625335\n","The percentage ZF reward = 167.82177977774168\n","The percentage mmse reward = 90.64468611068595\n","____Saving model of (logReward_constrain_lowSNR) version safe_3____\n","\n","\n","____Saving model of (logReward_constrain_lowSNR) version best_3____\n","episode 1 score MRT 90.8 score ZF 167.8 avg percentage 90.8\n","The percentage MRT reward = 90.88587493442212\n","The percentage ZF reward = 171.9035887771707\n","The percentage mmse reward = 90.69972771416484\n","____Saving model of (logReward_constrain_lowSNR) version safe_3____\n","\n","\n","____Saving model of (logReward_constrain_lowSNR) version best_3____\n","episode 2 score MRT 90.9 score ZF 171.9 avg percentage 90.9\n","The percentage MRT reward = 90.54123101575998\n","The percentage ZF reward = 177.69121668584762\n","The percentage mmse reward = 90.35203728592019\n","____Saving model of (logReward_constrain_lowSNR) version safe_3____\n","\n","\n","episode 3 score MRT 90.5 score ZF 177.7 avg percentage 90.9\n","The percentage MRT reward = 90.47201435928997\n","The percentage ZF reward = 174.58820218841433\n","The percentage mmse reward = 90.30984866241582\n","____Saving model of (logReward_constrain_lowSNR) version safe_3____\n","\n","\n","episode 4 score MRT 90.5 score ZF 174.6 avg percentage 90.9\n","____Saving model of (logReward_constrain_lowSNR) version At_end_3____\n"]}],"source":["evaluate = False\n","agent.check_path('plots/model_' + changing_alg)\n","\n","num_of_print = 1\n","best_ave = -np.inf\n","score_history = []\n","\n","# Initialize matrices for evaluations\n","states_mat_3d = np.zeros((n_episode, n_time_steps, n_users, n_tx * 2), dtype=np.float32)\n","\n","reward_mat = np.zeros((n_episode, num_of_print, n_users), dtype=np.float32)\n","ave_reward_mat_mrt = np.zeros((n_episode, num_of_print, n_users), dtype=np.float32)\n","ave_reward_mat_zf = np.zeros((n_episode, num_of_print, n_users), dtype=np.float32)\n","ave_reward_mat_mmse = np.zeros((n_episode,num_of_print,n_users),dtype=np.float32)\n","precoder_MRT = np.zeros((n_episode, num_of_print), dtype=np.float32)\n","precoder_zf = np.zeros((n_episode, num_of_print), dtype=np.float32)\n","precoder_mmse= np.zeros((n_episode,num_of_print),dtype=np.float32)\n","\n","# Start the episodes\n","for episode in range(n_episode):\n","    score = 0\n","    count = 0\n","    \n","    # Initialize channel state\n","    states = PAE(channel_states(n_users, n_tx, seed_number=0), n_tx)\n","    \n","    for time_step in range(n_time_steps):        \n","        # Choose action based on current state\n","        actions = agent.choose_action(states, evaluate)\n","        \n","        # Calculate reward\n","        reward_vector = get_reward_vectorize(states, actions, n_tx=n_tx, snr=snr_user)\n","        reward = np.dot(reward_vector, weight_rate)\n","\n","        # Store the current state\n","        states_3d = np.expand_dims(states.reshape((n_users, n_tx * 2)), axis=0)\n","        states_mat_3d[episode, time_step] = states_3d[0]\n","\n","        # Update SNR for the next time step\n","        next_states = PAE(channel_states(n_users, n_tx, seed_number=time_step, random=True), n_tx)\n","\n","        # Cumulative reward calculation\n","        score += reward  \n","        agent.remember(states, actions, reward, next_states)\n","        \n","        # Update states for the next iteration\n","        states = next_states\n","        \n","        if not evaluate:  # mean evaluate = False\n","            agent.learn()\n","\n","        # Evaluation section\n","        # We choose not to evaluate at every step, so the evaluation frequency is controlled by the num_of_print variable.\n","        if (time_step + 1) % (n_time_steps // num_of_print) == 0:\n","            state_for_eval = states_mat_3d[episode, 0:(time_step + 1)]\n","            ave_ac,ave_mrt,ave_zf,ave_mmse,p_mrt,p_zf,p_mmse = evaluate_while_training(state_for_eval, agent, snr_user)\n","\n","            reward_mat[episode, count] = ave_ac\n","            ave_reward_mat_mrt[episode, count] = ave_mrt\n","            ave_reward_mat_zf[episode, count] = ave_zf\n","            ave_reward_mat_mmse[episode,count] = ave_mmse\n","\n","            precoder_MRT[episode, count] = p_mrt\n","            precoder_zf[episode, count] = p_zf\n","            precoder_mmse[episode,count] = p_mmse\n","            \n","            # Print evaluation results\n","            print(f\"The percentage MRT reward = {p_mrt}\")\n","            print(f\"The percentage ZF reward = {p_zf}\")\n","            print(f\"The percentage mmse reward = {p_mmse}\")\n","            agent.save_models(name='safe_' + run_num)\n","            count += 1\n","            print(\"\\n\")\n","\n","    score_history.append(score)\n","\n","    # Save matrices for analysis\n","    np.save(f'{figure_file}reward_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', reward_mat)\n","    np.save(f'{figure_file}States_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', states_mat_3d)\n","    np.save(f'{figure_file}reward_MRT_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', ave_reward_mat_mrt)\n","    np.save(f'{figure_file}reward_ZF_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', ave_reward_mat_zf)\n","    np.save(f'{figure_file}percentage_mrt_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', precoder_MRT)\n","    np.save(f'{figure_file}percentage_zf_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', precoder_zf)\n","\n","    # Adjusting the agent's parameters to promote convergence\n","    # - Decaying the noise level, learning rate (alpha), and weight decay parameter (beta)\n","    #   by dividing them by the current iteration index plus one. \n","    #   This gradual reduction helps stabilize the training process and leads to better \n","    #   convergence towards the optimal solution.\n","    agent.noise = noise / (episode + 1)\n","    agent.alpha = alpha / (episode + 1)\n","    agent.beta = beta / (episode + 1)\n","\n","    if min(precoder_MRT[episode, -1], precoder_zf[episode, -1]) > best_ave:\n","        best_ave = min(precoder_MRT[episode, -1], precoder_zf[episode, -1])\n","        agent.save_models(name='best_' + run_num)\n","\n","    print('episode', episode, 'score MRT %.1f' % precoder_MRT[episode, -1], 'score ZF %.1f' % precoder_zf[episode, -1], 'avg percentage %.1f' % best_ave)\n","\n","# Save final model\n","agent.save_models(name='At_end_' + run_num)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.status.busy":"2023-07-11T02:22:11.097321Z","iopub.status.idle":"2023-07-11T02:22:11.097849Z","shell.execute_reply":"2023-07-11T02:22:11.097624Z","shell.execute_reply.started":"2023-07-11T02:22:11.097603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The value of Alpha at end =  4.000000000000001e-06\n","The value of  Beta at end =  8.000000000000001e-06\n","The value of noise at end =  4e-05\n"]}],"source":["print(\"The value of Alpha at end = \",agent.alpha)\n","print(\"The value of  Beta at end = \",agent.beta)\n","print(\"The value of noise at end = \",agent.noise)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"91536ba5-6450-4240-bdfa-d69a9146eda0","_uuid":"9741d99a-6166-45bb-b06a-c7d7c24e99c5","id":"IwHdfFddaHPn","trusted":true},"source":["# Load the agent"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9f9d6c5b-139e-4855-b129-8cf57c0debca","_uuid":"712ce7c9-321f-488f-bb52-01495b47f146","id":"vBUAd1zGpUdQ","trusted":true},"source":["### If in folder"]},{"cell_type":"code","execution_count":188,"metadata":{"_cell_guid":"c996032b-a966-41b6-979f-e3b3c80759ef","_uuid":"c076d483-41db-43f6-8bd6-57a251a492ec","collapsed":false,"execution":{"iopub.status.busy":"2023-07-11T02:22:11.100071Z","iopub.status.idle":"2023-07-11T02:22:11.100574Z","shell.execute_reply":"2023-07-11T02:22:11.100376Z","shell.execute_reply.started":"2023-07-11T02:22:11.100355Z"},"id":"OMU6LSvuhgHn","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["____Loading model of (general_snr) version best_2____\n"]}],"source":["agent.load_models(name='best_'+run_num)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0aefae70-91f6-4078-9edb-f4e781e9ebe1","_uuid":"ba44f5e2-f18b-4500-a83d-73f0eaa4cc66","id":"GA_l-DL2voZX","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Evaluate the agent"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9a125575-4627-46c3-8b01-a7b3362c0546","_uuid":"629f46af-09b2-4f7e-a455-0608586423de","id":"8DkEcpL5FtqT","trusted":true},"source":["## Graphs"]},{"cell_type":"code","execution_count":37,"metadata":{"_cell_guid":"59374f8e-767a-4f2f-b4e7-b57ae7126634","_uuid":"a8be6f34-0e8f-4ab2-9785-633f57bec411","collapsed":false,"execution":{"iopub.status.busy":"2023-07-11T02:22:11.104639Z","iopub.status.idle":"2023-07-11T02:22:11.105106Z","shell.execute_reply":"2023-07-11T02:22:11.104919Z","shell.execute_reply.started":"2023-07-11T02:22:11.104895Z"},"id":"ZHE4yrswFwLu","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Set evaluate flag to True to skip plotting\n","evaluate = True\n","\n","if not evaluate:\n","    # Title and filename for the first plot\n","    title1 = \"Percentage\"\n","    figure1_name = f'Percentage_{run_num}.png'\n","    \n","    # Plot the learning curve for MRT and ZF percentages\n","    plot_learning_curve(\n","        y_=[precoder_MRT[:, -1], precoder_zf[:, -1]],\n","        lim=n_episode,\n","        label=[\"Percentage MRT\", \"Percentage ZF\"],\n","        figure_file=figure_file + figure1_name,\n","        title=title1\n","    )\n","\n","    # Calculate the average rewards for MRT, ZF, and the model\n","    average_mrt = np.matmul(ave_reward_mat_mrt[:, -1], weight_rate)\n","    average_zf = np.matmul(ave_reward_mat_zf[:, -1], weight_rate)\n","    average_model = np.matmul(reward_mat[:, -1], weight_rate)\n","    \n","    # Title and filename for the second plot\n","    title2 = \"Average sum reward over episodes\"\n","    figure2_name = f'Average sum reward_{run_num}.png'\n","    \n","    # Plot the learning curve for average rewards\n","    plot_learning_curve(\n","        y_=[average_model, average_mrt, average_zf],\n","        lim=n_episode,\n","        label=[\"Actor\", \"MRT\", \"ZF\"],\n","        figure_file=figure_file + figure2_name,\n","        title=title2\n","    )\n","    \n","    # Title and filename for the third plot\n","    figure3_name = f'AverageOver_100_runNumIs_{run_num}.png'\n","    \n","    # Plot the learning curve for the score history\n","    plot_learning_curve(\n","        y_=[score_history],\n","        lim=n_episode,\n","        label=[\"a\"],\n","        figure_file=figure_file + figure3_name\n","    )"]},{"cell_type":"markdown","metadata":{"_cell_guid":"cc783749-a025-4d57-8967-c3852d332957","_uuid":"2c70cc51-5b9d-4fe9-b0e7-3ca3ee1b6910","id":"poZUk-2XF7Y1","trusted":true},"source":["## Numerical Evaluation\n","\n","Calculate the reward for a large number of instances using the vectorized function and take the average.\n","\n","1. **Generate or Define Instances**: Create a large dataset of instances (channel states) for which the reward needs to be calculated.\n","2. **Calculate the Precoder Matrix Using Actor**: Use the large dataset of instances to calculate the precoder matrix.\n","3. **Calculate Rewards**: Use the vectorized reward function to compute rewards for all instances at once.\n","4. **Average the Rewards**: Calculate the average reward across all instances."]},{"cell_type":"markdown","metadata":{},"source":["### High SNR"]},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"e0c834d9-ec87-4fe6-aae5-9cac151baa05","_uuid":"898357f6-1211-45ad-8df3-dbf8ce59a25c","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:13:34.938290Z","iopub.status.busy":"2023-07-11T18:13:34.937871Z","iopub.status.idle":"2023-07-11T18:13:35.014242Z","shell.execute_reply":"2023-07-11T18:13:35.013342Z","shell.execute_reply.started":"2023-07-11T18:13:34.938257Z"},"id":"Q8Ee8-7kL5wR","jupyter":{"outputs_hidden":false},"outputId":"a4895fcb-16de-404f-934a-d4f5e7837429","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average reward by actor =\t [1.5799184 1.5734459 1.6192862 1.698712 ]\n","Average reward by MRT   =\t [1.7174187 1.7048019 1.6969966 1.6832829]\n","Average reward by ZF    =\t [4.7038627 4.71586   4.695405  4.708968 ]\n","Average reward by MMSE   =\t [4.767459  4.775095  4.7577763 4.7699056]\n","Percentage subject to MRT =\t 95.13211140121284 %\n","Percentage subject to ZF  =\t 34.378075208972355 %\n","Percentage subject to MMSE =\t 33.93435741450343 %\n"]}],"source":["n_time_steps_eval = 1000\n","\n","# Generate a channel matrix without the PAE effect\n","H_without_pae = channel_matrix(n_time_steps_eval, n_tx=n_tx, random=True)\n","# H_without_pae = channel_matrix(n_time_steps_eval, n_tx=n_tx, seed_number=400)\n","\n","# Apply the PAE effect to the channel matrix\n","H = PAE_3d(H_without_pae)\n","\n","# Compute the precoders for MRT, ZF, and MMSE\n","W_mrt = MRT(H)\n","W_zf = zf_mat(H)\n","W_mmse = mmse_mat(H, snr=snr_user)\n","\n","# Calculate the rewards for each precoder\n","reward_mrt = reward_for_3d(H, W_mrt, snr=snr_user)\n","reward_zf = reward_for_3d(H, W_zf, snr=snr_user)\n","reward_mmse = reward_for_3d(H, W_mmse, snr=snr_user)\n","\n","# Choose action using the agent based on the current state and evaluate\n","action_mat = agent.choose_action(H.reshape((H.shape[0],1,-1)), evaluate=True)\n","action_mat = tf.reshape(action_mat, (n_time_steps_eval, n_users, -1))\n","\n","# Calculate the evaluation rewards based on the chosen action\n","eva_reward_mat = reward_for_3d(H, tf.reshape(action_mat, (H.shape[0], n_users, -1)), snr=snr_user)\n","\n","# Compute the mean rewards for MRT, ZF, MMSE, and the model's actions\n","rmrt = np.mean(reward_mrt, axis=0)\n","rzf = np.mean(reward_zf, axis=0)\n","rmmse = np.mean(reward_mmse, axis=0)\n","rmod = np.mean(eva_reward_mat, axis=0)\n","\n","# Print the average rewards and performance metrics\n","print(f\"Average reward by actor =\\t {rmod}\")\n","print(f\"Average reward by MRT   =\\t {rmrt}\")\n","print(f\"Average reward by ZF    =\\t {rzf}\")\n","print(f\"Average reward by MMSE   =\\t {rmmse}\")\n","print(f\"Percentage subject to MRT =\\t {np.sum(rmod) * 100.0 / np.sum(rmrt)} %\")\n","print(f\"Percentage subject to ZF  =\\t {np.sum(rmod) * 100.0 / np.sum(rzf)} %\")\n","print(f\"Percentage subject to MMSE =\\t {np.sum(rmod) * 100.0 / np.sum(rmmse)} %\")"]},{"cell_type":"markdown","metadata":{},"source":["### Single Instance (State) Evaluation"]},{"cell_type":"code","execution_count":39,"metadata":{"_cell_guid":"6a788d53-bf9d-4f81-88de-db2f69e598a8","_uuid":"5fde2a48-3192-4136-9cda-67d8c3d88f8d","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:13:24.456818Z","iopub.status.busy":"2023-07-11T18:13:24.456423Z","iopub.status.idle":"2023-07-11T18:13:24.478464Z","shell.execute_reply":"2023-07-11T18:13:24.477076Z","shell.execute_reply.started":"2023-07-11T18:13:24.456787Z"},"id":"tOyD7g5VRjsD","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reward by actor =\t [0.10820762 0.05179625 0.12350858 0.06635842]\n","Reward by MRT   =\t [0.129981   0.05977865 0.12960957 0.08277049]\n","Reward by ZF    =\t [0.06295161 0.02585701 0.10064117 0.06044395]\n","Reward by MMSE  =\t [0.08865417 0.03249981 0.10822549 0.06965037]\n","Percentage subject to MRT  =\t 87.0023194011903 %\n","Percentage subject to ZF   =\t 140.00786161834372 %\n","Percentage subject to MMSE  =\t 117.00199406484657 %\n"]}],"source":["# Generate channel states using the PAE effect with a fixed seed for reproducibility\n","h = PAE(channel_states(n_tx=n_tx, seed_number=42, random=True))\n","\n","# Reshape the channel states to 3D for processing\n","h_3d = np.expand_dims(h.reshape((n_users, n_tx * 2)), axis=0)\n","\n","# Calculate rewards using different precoding techniques\n","reward_mrt_sample = reward_for_3d(h_3d, MRT(h_3d), snr=snr_user)[0]\n","reward_zf_sample = reward_for_3d(h_3d, zf_mat(h_3d), snr=snr_user)[0]\n","reward_mmse_sample = reward_for_3d(h_3d, mmse_mat(h_3d), snr=snr_user)[0]\n","\n","# Choose action using the agent based on the current state and evaluate\n","precoder = agent.choose_action(h, evaluate=True)\n","\n","# Calculate the reward for the chosen action\n","reward_model_sample = get_reward_vectorize(h, precoder, snr=snr_user)\n","\n","# Print the rewards obtained from the actor and different methods\n","print(f\"Reward by actor =\\t {reward_model_sample}\")\n","print(f\"Reward by MRT   =\\t {reward_mrt_sample}\")\n","print(f\"Reward by ZF    =\\t {reward_zf_sample}\")\n","print(f\"Reward by MMSE  =\\t {reward_mmse_sample}\")\n","\n","# Calculate and print the percentage performance relative to each method\n","print(f\"Percentage subject to MRT  =\\t {np.sum(reward_model_sample) * 100.0 / np.sum(reward_mrt_sample)} %\")\n","print(f\"Percentage subject to ZF   =\\t {np.sum(reward_model_sample) * 100.0 / np.sum(reward_zf_sample)} %\")\n","print(f\"Percentage subject to MMSE  =\\t {np.sum(reward_model_sample) * 100.0 / np.sum(reward_mmse_sample)} %\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3498260,"sourceId":6106608,"sourceType":"datasetVersion"},{"datasetId":3504405,"sourceId":6114834,"sourceType":"datasetVersion"},{"sourceId":135953607,"sourceType":"kernelVersion"}],"dockerImageVersionId":30513,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":4}
