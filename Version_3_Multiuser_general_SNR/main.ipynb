{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c46817c8-5917-4a94-a77a-1c6c589a626b","_uuid":"4c968fd4-baba-4513-ab2a-d9535925f7e0","id":"QCHK-aChxg3G","trusted":true},"source":["## General SNR as Input Version\n","\n","This version of the project focuses on integrating Signal-to-Noise Ratio (SNR) as an input to the Deep Reinforcement Learning (DRL) model for optimal precoder vector design and power allocation in Multi-User MISO (MU-MISO) systems. By considering SNR as a variable input, the model can adapt its precoding strategy to varying channel conditions, thereby enhancing overall system performance.\n","\n","### Objectives\n","- **Adaptation to Channel Conditions:** Allow the model to dynamically adjust its precoding techniques based on real-time SNR values, improving robustness and performance in diverse environments.\n","- **Model Training:** Train the DRL agent under different SNR scenarios, ensuring that it can effectively learn optimal strategies across a range of signal conditions.\n","- **Performance Comparison:** Evaluate the performance of the SNR-aware model against traditional linear precoders such as MRT, ZF, and MMSE to highlight the advantages of the DRL approach in varying SNR conditions.\n","\n","### Implementation Details\n","- **Input Configuration:** The model's architecture is modified to accept SNR as a key input parameter, enabling it to better understand the impact of noise on signal transmission.\n","- **Training Process:** The DRL agent is trained through simulated environments with varying SNR levels, ensuring that it can learn optimal precoding strategies that account for these variations.\n","- **Results Analysis:** Analyze and compare the results obtained from this version with those from other precoding methods to assess improvements in performance metrics such as throughput and bit error rate (BER).\n","\n","This version demonstrates the versatility and effectiveness of incorporating SNR as an input in the design of optimal precoding solutions for MU-MISO systems."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import Packeges\n","import numpy as np\n","import tensorflow as tf\n","import os\n","\n","from DDPG_MultiUser import Agent\n","from utils import plot_learning_curve"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"231aa792-d754-4127-8747-67c1e2c53185","_uuid":"ab7196a9-c728-4d8f-a583-db8621e78c35","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:00:08.401581Z","iopub.status.busy":"2023-07-11T18:00:08.401260Z","iopub.status.idle":"2023-07-11T18:00:08.438806Z","shell.execute_reply":"2023-07-11T18:00:08.438034Z","shell.execute_reply.started":"2023-07-11T18:00:08.401555Z"},"id":"r1HskVuwriq4","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# We use this to build the first run based on the results of the previous run.\n","changing_alg = 'general_snr'\n","prev_run_num = '2'\n","run_num = '3'\n","figure_file = 'plots/model_'+changing_alg+'/'"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"5a3e988d-8baf-4b1e-8ae0-df9d275bf61f","_uuid":"b50867f8-d79a-4eb6-ba2d-3c43bb432d17","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:00:08.440779Z","iopub.status.busy":"2023-07-11T18:00:08.439895Z","iopub.status.idle":"2023-07-11T18:00:08.444850Z","shell.execute_reply":"2023-07-11T18:00:08.444010Z","shell.execute_reply.started":"2023-07-11T18:00:08.440751Z"},"id":"PmIKDxThTVE2","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["## MISO multi user \n","n_tx = 6   # number of atennas\n","n_users = 4 # number of users\n","# This is the importance of each user  \n","weight_rate = np.full((n_users,),0.25)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a191aafe-2ad5-4f3f-8590-4f4552dd6ed0","_uuid":"3eca2d51-b369-45c3-b336-1d434be63e19","id":"bJPKeJFivoZR","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Helpful functions"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def turn_mat_to_complex(matrix,n_tx = 6):\n","    \"\"\"Turn real matrix to complex matrix.\"\"\"\n","    if matrix.shape[0] ==1:\n","        matrix = np.reshape(matrix,(-1,2*n_tx))\n","    return matrix[:,:n_tx]+ 1j*matrix[:,n_tx:]\n","\n","def PAE(states,n_tx = 6):\n","    \"\"\"Phase ambiguity elimination to reach a unique solution.\"\"\"\n","    h_ = turn_mat_to_complex(states,n_tx)\n","    h_zero_phase = h_*np.reshape(np.exp(-1j*np.angle(h_))[:,0],(-1,1))\n","    h_pae_row = np.concatenate([np.real(h_zero_phase),np.imag(h_zero_phase)],axis=1).reshape((1,-1))\n","    return h_pae_row"]},{"cell_type":"markdown","metadata":{},"source":["## Channel Model\n","\n","The channel model can be represented as:\n","\n","$$\n","\\mathbf{Y} = \\sqrt{\\rho} \\cdot \\mathbf{H} \\mathbf{W}^H \\mathbf{s} + \\mathbf{n}\n","$$\n","\n","where:\n","\n","- ρ is the input SNR, defined as ρ = Pt / σn².\n","- H is a matrix with dimensions K x n_tx.\n","\n","### Definitions:\n","\n","- K: Number of users.\n","- n<sub>tx</sub>: Number of antennas at the base station (BS)."]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"720efede-13db-4988-b52f-c79b00823002","_uuid":"aacb5dfb-e04b-45cd-a5f2-26a2d7db71ac","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:00:18.126079Z","iopub.status.busy":"2023-07-11T18:00:18.125103Z","iopub.status.idle":"2023-07-11T18:00:18.137908Z","shell.execute_reply":"2023-07-11T18:00:18.136512Z","shell.execute_reply.started":"2023-07-11T18:00:18.126049Z"},"id":"cjcQNZ34voZS","jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["def channel_states(n_users = 4, n_tx = 6, seed_number = 42 , random = False):\n","    \"\"\"Channel model.\"\"\"\n","    # H represents the channel states and has a size of (# of users, # of transmitted antennas).\n","    if not random:\n","        np.random.seed(seed_number)\n","    real =(1/np.sqrt(2))  * np.random.randn(n_users,n_tx).astype(np.float32)  # 4x 6\n","    imag = (1/np.sqrt(2)) * np.random.randn(n_users,n_tx).astype(np.float32) # 4x6\n","    H = np.hstack([real , imag]) #4x 12\n","    H = np.reshape(H,(1,-1)) #1 x 12*4(48)\n","    return H"]},{"cell_type":"markdown","metadata":{},"source":["# The Reward Equation\n","\n","### The reward equation is given by:\n","\n","$$\n","\\text{Reward} = \\sum_{k=1}^{K} (\\nu_k R_k) = \\sum_{k=1}^{K} \\left( \\nu_k \\cdot \\log_2 \\left( 1 + \\frac{(\\rho |h_k \\cdot w_k^H|)^2}{1 + \\rho \\sum_{j=1, j \\neq k}^{K} |h_k \\cdot w_j^H|^2} \\right) \\right)\n","$$\n","\n","### Where:\n","- ρ is the input SNR\n","- K: Number of users.\n","- n<sub>tx</sub>: Number of antennas at the base station (BS).\n","- h: Channel state.\n","- w: Precoder vector."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","def get_reward_vectorize(h_estimate, precoder_mat,n_tx = 6,snr = 1.0):\n","    \"\"\"Get reward as a vector.\"\"\"\n","    # Reward equation sum weighted rate of users.\n","    h_estimate = turn_mat_to_complex(h_estimate,n_tx) # (# of users, # of transmitted antennas)\n","    \n","    precoder_mat = turn_mat_to_complex(precoder_mat,n_tx) # (# of users, # of transmitted antennas)\n","    precoder_mat_H = (precoder_mat).conjugate().T  # (# of transmitted antennas, # of users) due to .T\n","    ## Multiplication of the channel estimate matrix by the precoder vector.\n","    #   - The diagonal elements represent the power without interference, \n","    #     so hi_py_wi[i, i] is the power at user i.\n","    #   - The off-diagonal elements represent the interference, \n","    #     so hi_py_wi[i, j] where i ≠ j represents the interference at user i.\n","    hi_py_wi = snr*np.square(np.abs(np.matmul(h_estimate,precoder_mat_H))) \n","    power = np.diag(hi_py_wi) \n","    interference = hi_py_wi.copy()\n","    # Make diognal zero so sum of each Row is the interference only\n","    np.fill_diagonal(interference,0) \n","\n","    interference = 1 +np.sum(interference,axis=1)\n","    reward_vector =  np.log2(1+power/interference)\n","    return reward_vector\n","\n","\n","\n","def state_plus_snr(state, snr =1.0):\n","    \"\"\"Appends the SNR (Signal-to-Noise Ratio) to the state input, allowing the output to vary based on the SNR value.\"\"\"\n","    if(state.shape[0]!=1):\n","        state = np.reshape(state,(1,-1))\n","    states_mod = np.insert(state, state.shape[1], snr, axis=1)\n","    return states_mod"]},{"cell_type":"markdown","metadata":{"_cell_guid":"12fcf2a3-592d-4043-99f2-8c1c1c777ecc","_uuid":"ae63f490-1897-441a-9ba8-7852925ae43b","id":"qGAehqg-Fj34","trusted":true},"source":["# Function for Evaluation\n","This function is highly optimized with vectorization, allowing it to run very quickly.\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def turn_3d_to_C(csi,n_tx = 6):\n","    \"\"\"Convert a 3D real array into a complex array.\"\"\"\n","    # CSI stands for Channel State Information.\n","    #   csi.shape[0] is the number of instances I want to evaluate.\n","    matrix = np.reshape(csi,(csi.shape[0],-1,2*n_tx))\n","    return matrix[:,:,:n_tx]+ 1j*matrix[:,:,n_tx:]\n","\n","def PAE_3d(H,n_tx = 6):\n","    \"\"\"Perform phase ambiguity elimination for a 3D matrix.\"\"\"\n","    H_c = turn_3d_to_C(H,n_tx)\n","    H_zero_phase = H_c*np.exp(-1j*np.expand_dims(np.angle(H_c)[:,:,0],axis=2))\n","    h_pae_row = np.concatenate([np.real(H_zero_phase),np.imag(H_zero_phase)],axis=2)\n","    return h_pae_row"]},{"cell_type":"markdown","metadata":{},"source":["## Generate H Matrix\n","\n","3-D matrix dimensions: (number of instances, number of users, number of transmitted antennas)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def channel_mat_seed(n_runs =5000,n_users = 4, n_tx = 6, lower_seed = 0):\n","    \"\"\"Generate an H matrix where each row is initialized with a different seed.\"\"\"\n","    matrix = np.zeros((n_runs,n_users,n_tx*2),dtype=np.float32)\n","    for i in range(lower_seed,n_runs + lower_seed):\n","        np.random.seed(i)\n","        real =(1/np.sqrt(2))  * np.random.randn(n_users,n_tx).astype(np.float32)\n","        imag = (1/np.sqrt(2)) * np.random.randn(n_users,n_tx).astype(np.float32)\n","        matrix[i-lower_seed] = np.hstack([real , imag])\n","    return matrix\n","\n","def channel_matrix(n_evaluation ,n_users = 4,n_tx= 6, seed_number = 42 ,random = False):\n","    \"\"\"Generate an H matrix where each row is fully random and initialized with a different seed.\"\"\"\n","    if not random:\n","        np.random.seed(seed_number)\n","    real =(1/np.sqrt(2))  * np.random.randn(n_evaluation,n_users,n_tx).astype(np.float32)\n","    imag = (1/np.sqrt(2)) * np.random.randn(n_evaluation,n_users,n_tx).astype(np.float32)\n","    H = np.dstack([real , imag])\n","    # H = np.reshape(H,(n_evaluation,1,-1))\n","    return H"]},{"cell_type":"markdown","metadata":{},"source":["## Function to Calculate Reward for 3D H Matrix\n","\n","This function calculates the reward for a 3D channel states matrix.\n","\n","**Parameters:**\n","\n","- **H**: A 3D channel states matrix with dimensions (number of instances, number of users, number of transmitted antennas).\n","- **W**: A precoder matrix with the same dimensions as H.\n","\n","**Returns:**\n","\n","- A reward matrix with dimensions (number of instances, number of users) for all instances and users."]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"5508d5a1-3b60-46d3-bb2c-b42e96f5c1c7","_uuid":"426758c7-be37-4ff9-ae07-76a44ddb6295","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:02:22.940620Z","iopub.status.busy":"2023-07-11T18:02:22.940190Z","iopub.status.idle":"2023-07-11T18:02:23.192353Z","shell.execute_reply":"2023-07-11T18:02:23.190508Z","shell.execute_reply.started":"2023-07-11T18:02:22.940592Z"},"id":"G2a1oA75zUy6","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def reward_for_3d(H_3d, precoder_3d,snr = 1.0):\n","    \"\"\"Compute the reward function for multiple instances using vectorization to speed up computations.\"\"\"\n","    n_tx = H_3d.shape[2]//2   ## Number of transmitted antennas\n","    \n","    ### Turn 3d matrix to Comlex\n","    h_estimate = turn_3d_to_C(H_3d,n_tx) # Has size (n_runs x n_user x n_tx)\n","    precoder_3d = turn_3d_to_C(precoder_3d,n_tx)\n","    # Transpose along the y and z axes.\n","    precoder_3d_t = np.transpose(precoder_3d.conjugate(),axes=[0 ,2 ,1]) # Has size (n_runs x n_tx x n_user)\n","    ### Get all combination of   |hi*wi|**2\n","    H_py_W = snr*np.square(np.abs(np.matmul(h_estimate,precoder_3d_t))) # Has size (n_runs x n_user x n_user)\n","    \n","    ### Diagonal elements\n","    power = np.diagonal(H_py_W,axis1=1,axis2=2)  # Has size (n_runs x n_user )\n","    ###  Fill diagonal with zeros\n","    interference = H_py_W.copy()\n","    diagonal_index = np.arange(H_3d.shape[1])\n","    interference[:, diagonal_index, diagonal_index] = 0\n","    ### Get the off diagonal elments\n","    interference = 1.0 +np.sum(interference,axis=2)\n","    reward_vector =  np.log2(1+power/interference)\n","    return reward_vector\n","\n","def state_3d_snr(state, snr):\n","    \"\"\"Appends the SNR (Signal-to-Noise Ratio) to the state input, allowing the output to vary based on the SNR value.\"\"\"\n","    if(state.shape[0]!=1):        \n","        state = state.reshape((state.shape[0],1,-1))\n","    # Convert the SNR to a vector if the SNR parameter is provided as a scalar value.\n","    if(np.isscalar(snr)):\n","        snr = np.ones((state.shape[0],1,1),dtype= np.float32)*snr\n","    state_mod = np.concatenate([state,snr],axis=2)\n","    return state_mod"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Precoder Techniques to Evaluate Our Model\n","\n","1. **MRT**  (Maximum Ratio Transmission)\n","2. **ZF**   (Zero-Forcing)\n","3. **MMSE** (Minimum Mean Square Error)\n","\n","The precoder matrices are defined as follows:\n","\n","$$\n","\\begin{align*}\n","W_{MRT} & = H \\\\\n","W_{ZF} & = H(H^H H)^{-1} \\\\\n","W_{MMSE} & = H(H^H H + \\frac{1}{\\rho} I_K)\n","\\end{align*}\n","$$"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def sum_norm_root_square(H):\n","    \"\"\"Calculate the summation of the squared norm of the channel matrix H.\"\"\"\n","    sum_norm = np.sqrt(np.sum(np.square(np.linalg.norm(H,axis=2)),axis=1))\n","    sum_norm = np.repeat(sum_norm,H.shape[1]).reshape(H.shape[0],H.shape[1])  ### To help us on Broadcast\n","    sum_norm = np.expand_dims(sum_norm,axis=2)       ### has n_runs x n_users x1 dimentions\n","    return sum_norm\n","\n","def MRT(H):  ### n_runs x n_user x n_tx*2\n","    \"\"\"MRT stands for Maximum Ratio Transmission, a linear precoder technique.\"\"\"\n","    sum_norm = sum_norm_root_square(H)\n","    precoder  = H/sum_norm\n","    return precoder\n","\n","def zf_mat(H):\n","    \"\"\"ZF stands for Zero-Forcing, a linear precoder technique.\"\"\"\n","    n_tx = H.shape[2]//2\n","    H = turn_3d_to_C(H,n_tx=n_tx)\n","    zf_H = np.matmul(np.linalg.pinv(np.matmul(H,np.transpose(H.conjugate(),axes=[0,2,1]))),H)\n","    sum_norm = sum_norm_root_square(zf_H)\n","    # sum_norm = 1.0\n","    precoder_c  = zf_H/sum_norm\n","    precoder = np.concatenate([np.real(precoder_c),np.imag(precoder_c)],axis=2)\n","    return precoder\n","\n","def mmse_mat(H,snr= 1.0):\n","    \"\"\"MMSE stands for Minimum Mean Square Error, a linear precoder technique.\"\"\"\n","    n_tx = H.shape[2]//2\n","    H = turn_3d_to_C(H,n_tx=n_tx)\n","    identity_3d = np.zeros((H.shape[0],H.shape[1],H.shape[1]),dtype=np.float32)\n","    idx = np.arange(H.shape[1])\n","    identity_3d[:,idx,idx] = 1/snr\n","    mmse_H = np.matmul(np.linalg.inv(np.matmul(H,np.transpose(H.conjugate(),axes=[0,2,1]))+identity_3d),H)\n","    sum_norm = sum_norm_root_square(mmse_H)\n","    precoder_c  = mmse_H/sum_norm\n","    precoder = np.concatenate([np.real(precoder_c),np.imag(precoder_c)],axis=2)\n","    return precoder"]},{"cell_type":"markdown","metadata":{},"source":["### Put all together."]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"74343291-7568-4e47-9e05-043e0954f92a","_uuid":"df9a9a2c-1614-4ef9-9348-930c6eb3cfa2","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:02:23.194643Z","iopub.status.busy":"2023-07-11T18:02:23.194269Z","iopub.status.idle":"2023-07-11T18:02:23.211641Z","shell.execute_reply":"2023-07-11T18:02:23.210344Z","shell.execute_reply.started":"2023-07-11T18:02:23.194620Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def evaluate_while_training(state_for_eval, agent, snr_user):\n","    \"\"\"Compare the predicted results with those obtained from other linear techniques.\"\"\"\n","    snr_user = np.expand_dims(snr_user,axis= 2)\n","    state_mod = state_3d_snr(state_for_eval,snr_user)\n","    cumulative_reward_mrt = reward_for_3d(state_for_eval,MRT(state_for_eval), snr= snr_user)\n","    cumulative_reward_zf = reward_for_3d(state_for_eval,zf_mat(state_for_eval), snr= snr_user)\n","    action_mat_in = tf.reshape(agent.choose_action(state_mod, evaluate = True),(state_for_eval.shape[0],n_users,-1))\n","    reward_mat_in = reward_for_3d(state_for_eval,action_mat_in, snr= snr_user)\n","    ave_reward_actor   = np.mean(reward_mat_in,axis =0)\n","    ave_reward_mat_mrt = np.mean(cumulative_reward_mrt,axis =0)\n","    ave_reward_mat_zf  = np.mean(cumulative_reward_zf,axis =0)\n","    percentage_mrt = np.sum(np.mean(reward_mat_in,axis =0))*100.0/np.sum(ave_reward_mat_mrt)\n","    percentage_zf = np.sum(np.mean(reward_mat_in,axis =0))*100.0/np.sum(ave_reward_mat_zf)\n","    return ave_reward_actor, ave_reward_mat_mrt, ave_reward_mat_zf ,percentage_mrt, percentage_zf"]},{"cell_type":"markdown","metadata":{"_cell_guid":"73bca18f-1cd2-435d-bc99-33fda545eee6","_uuid":"6e3df2d3-f37e-4d91-819f-34fb069822ed","id":"D0RFplI5voZU","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Agent Initialization\n","\n","This code initializes an instance of the `Agent` class with the following parameters:\n","\n","- **input_dims**: A tuple representing the dimensions of the input, calculated as (2 x number of antennas x number of users + 1).\n","Adding 1 because we enter the SNR as an input.\n","- **alpha**: Learning rate, set to 3.3e-5.\n","- **beta**: Weight decay parameter, set to 6.7e-5.\n","- **gamma**: Discount factor, set to 0.1.\n","- **n_users**: The number of users.\n","- **n_tx**: The number of transmitted antennas.\n","- **max_size**: The maximum size of the replay buffer, set to 1,000,000.\n","- **tau**: Soft update parameter, set to 0.005.\n","- **fc1**, **fc2**, **fc3**: Number of neurons in the first, second, and third fully connected layers, set to 1024, 512, and 128, respectively.\n","- **batch_size**: Size of the mini-batch for training, set to 64.\n","- **noise**: Noise of the channel, set to 3.0e-4.\n","- **changing_alg**: The name of the path to save the model.\n","\n","The agent will use these parameters to interact with the environment and learn an optimal policy.\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["agent = Agent(\n","    input_dims=(2 * n_tx * n_users + 1,),\n","    alpha=3.3e-5,\n","    beta=6.7e-5,\n","    gamma=0.1,\n","    n_users=n_users,\n","    n_tx=n_tx,\n","    max_size=1000000,\n","    tau=0.005,\n","    fc1=1024,\n","    fc2=512,\n","    fc3=128,\n","    batch_size=64,\n","    noise=3.0e-4,\n","    changing_alg=changing_alg\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["alpha = agent.alpha \n","beta = agent.beta\n","noise = agent.noise"]},{"cell_type":"markdown","metadata":{"_cell_guid":"43167bc8-ae15-47c9-86de-36a90e398439","_uuid":"b6c46a92-3244-4a11-ab5b-45925cfd586a","id":"Wppie83VvoZV","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["### Loading Previous Weights\n","\n","This section is responsible for loading the previous weights, allowing the agent to resume learning from the results of the previous run. This functionality can help improve the convergence of the learning process by leveraging previously acquired knowledge. -->"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T18:02:27.483466Z","iopub.status.busy":"2023-07-11T18:02:27.483067Z","iopub.status.idle":"2023-07-11T18:02:27.933205Z","shell.execute_reply":"2023-07-11T18:02:27.931666Z","shell.execute_reply.started":"2023-07-11T18:02:27.483438Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["____Loading model of (general_snr) version best_2____\n"]}],"source":["### uncomment to be suitble in kaggle\n","# path = agent.model_path\n","# agent.model_path = \"../input/miso-mu-general/ddpg_weights/general_snr\"\n","if(os.path.exists(agent.model_path)):\n","    agent.learn(evaluate=True)\n","    agent.load_models(name='best_'+prev_run_num)\n","# agent.model_path = path"]},{"cell_type":"markdown","metadata":{"_cell_guid":"09a9eaf4-0d3c-4b1f-a9a0-8e43bdf83895","_uuid":"f28ef598-d47c-4f19-b81f-49c6aa5a6fd7","id":"QZEl3miOvoZW","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Training the agent"]},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"0fb60fdb-cca7-4079-90ac-5bb5ff1e2b6b","_uuid":"bca43750-5335-4420-bbb6-152b9c9d9b04","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T02:21:41.876513Z","iopub.status.busy":"2023-07-11T02:21:41.876144Z","iopub.status.idle":"2023-07-11T02:21:41.890510Z","shell.execute_reply":"2023-07-11T02:21:41.889088Z","shell.execute_reply.started":"2023-07-11T02:21:41.876483Z"},"id":"Ch8j1pPqB7UQ","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["n_episode = 5\n","n_time_steps = 100 "]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"b0e31da0-f6de-4493-a8cf-90ccf72cf46f","_uuid":"b99e0f53-cf9d-4738-8303-61ce15d0e56c","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T02:21:41.893685Z","iopub.status.busy":"2023-07-11T02:21:41.893259Z","iopub.status.idle":"2023-07-11T02:22:11.095429Z","shell.execute_reply":"2023-07-11T02:22:11.093444Z","shell.execute_reply.started":"2023-07-11T02:21:41.893650Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The percentage MRT reward = 78.81691229974723\n","The percentage ZF reward = 70.52722596665484\n","____Saving model of (general_snr) version safe_4____\n","\n","\n","____Saving model of (general_snr) version best_4____\n","episode 0 score MRT 78.8 score ZF 70.5 avg percentage 70.5\n","The percentage MRT reward = 81.0492315848517\n","The percentage ZF reward = 71.63508381884705\n","____Saving model of (general_snr) version safe_4____\n","\n","\n","____Saving model of (general_snr) version best_4____\n","episode 1 score MRT 81.0 score ZF 71.6 avg percentage 71.6\n","The percentage MRT reward = 84.04288375256662\n","The percentage ZF reward = 77.88825139193074\n","____Saving model of (general_snr) version safe_4____\n","\n","\n","____Saving model of (general_snr) version best_4____\n","episode 2 score MRT 84.0 score ZF 77.9 avg percentage 77.9\n","The percentage MRT reward = 82.68274337387047\n","The percentage ZF reward = 82.99408057910304\n","____Saving model of (general_snr) version safe_4____\n","\n","\n","____Saving model of (general_snr) version best_4____\n","episode 3 score MRT 82.7 score ZF 83.0 avg percentage 82.7\n","The percentage MRT reward = 81.96623664490969\n","The percentage ZF reward = 83.2788430227221\n","____Saving model of (general_snr) version safe_4____\n","\n","\n","episode 4 score MRT 82.0 score ZF 83.3 avg percentage 82.7\n","____Saving model of (general_snr) version At_end_4____\n"]}],"source":["evaluate = False\n","agent.check_path('plots/model_' + changing_alg)\n","\n","num_of_print = 1\n","best_ave = -np.inf\n","score_history = []\n","\n","# Initialize matrices for evaluations\n","states_mat_3d = np.zeros((n_episode, n_time_steps, n_users, n_tx * 2), dtype=np.float32)\n","snr = np.zeros((n_episode, n_time_steps, 1), dtype=np.float32)\n","\n","reward_mat = np.zeros((n_episode, num_of_print, n_users), dtype=np.float32)\n","ave_reward_mat_mrt = np.zeros((n_episode, num_of_print, n_users), dtype=np.float32)\n","ave_reward_mat_zf = np.zeros((n_episode, num_of_print, n_users), dtype=np.float32)\n","precoder_MRT = np.zeros((n_episode, num_of_print), dtype=np.float32)\n","precoder_zf = np.zeros((n_episode, num_of_print), dtype=np.float32)\n","\n","# Start the episodes\n","for episode in range(n_episode):\n","    score = 0\n","    count = 0\n","    snr_user = np.power(10, np.random.uniform(-1, 1))\n","    \n","    # Initialize channel states\n","    states = PAE(channel_states(n_users, n_tx, seed_number=0), n_tx)\n","    states_mod = state_plus_snr(states, snr_user)\n","    \n","    for time_step in range(n_time_steps):\n","        snr[episode, time_step] = snr_user\n","        \n","        # Choose action based on current state\n","        actions = agent.choose_action(states_mod, evaluate)\n","        \n","        # Calculate reward\n","        reward_vector = get_reward_vectorize(states, actions, n_tx=n_tx, snr=snr_user)\n","        reward = np.dot(reward_vector, weight_rate)\n","\n","        # Store the current state\n","        states_3d = np.expand_dims(states.reshape((n_users, n_tx * 2)), axis=0)\n","        states_mat_3d[episode, time_step] = states_3d[0]\n","\n","        # Update SNR for the next time step\n","        snr_user = np.power(10, np.random.uniform(-1, 1))\n","        next_states = PAE(channel_states(n_users, n_tx, seed_number=time_step, random=True), n_tx)\n","        next_states_mod = state_plus_snr(next_states, snr_user)\n","\n","        # Cumulative reward calculation\n","        score += reward  \n","        agent.remember(states_mod, actions, reward, next_states_mod)\n","        \n","        # Update states for the next iteration\n","        states = next_states\n","        states_mod = next_states_mod\n","        \n","        if not evaluate:  # mean evaluate = False\n","            agent.learn()\n","\n","        # Evaluation section\n","        # We choose not to evaluate at every step, so the evaluation frequency is controlled by the num_of_print variable.\n","        if (time_step + 1) % (n_time_steps // num_of_print) == 0:\n","            snr_for_eval = snr[episode, 0:(time_step + 1)]\n","            state_for_eval = states_mat_3d[episode, 0:(time_step + 1)]\n","            ave_ac, ave_mrt, ave_zf, p_mrt, p_zf = evaluate_while_training(state_for_eval, agent, snr_for_eval)\n","\n","\n","            reward_mat[episode, count] = ave_ac\n","            ave_reward_mat_mrt[episode, count] = ave_mrt\n","            ave_reward_mat_zf[episode, count] = ave_zf\n","            precoder_MRT[episode, count] = p_mrt\n","            precoder_zf[episode, count] = p_zf\n","            \n","            # Print evaluation results\n","            print(f\"The percentage MRT reward = {p_mrt}\")\n","            print(f\"The percentage ZF reward = {p_zf}\")\n","            agent.save_models(name='safe_' + run_num)\n","            count += 1\n","            print(\"\\n\")\n","\n","    score_history.append(score)\n","\n","    # Save matrices for analysis\n","    np.save(f'{figure_file}reward_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', reward_mat)\n","    np.save(f'{figure_file}States_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', states_mat_3d)\n","    np.save(f'{figure_file}reward_MRT_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', ave_reward_mat_mrt)\n","    np.save(f'{figure_file}reward_ZF_Matrix_over_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', ave_reward_mat_zf)\n","    np.save(f'{figure_file}percentage_mrt_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', precoder_MRT)\n","    np.save(f'{figure_file}percentage_zf_{n_episode}_{n_time_steps}_time_step_{changing_alg}_run_{run_num}.npy', precoder_zf)\n","\n","    # Adjusting the agent's parameters to promote convergence\n","    # - Decaying the noise level, learning rate (alpha), and weight decay parameter (beta)\n","    #   by dividing them by the current iteration index plus one. \n","    #   This gradual reduction helps stabilize the training process and leads to better \n","    #   convergence towards the optimal solution.\n","    agent.noise = noise / (episode + 1)\n","    agent.alpha = alpha / (episode + 1)\n","    agent.beta = beta / (episode + 1)\n","\n","    if min(precoder_MRT[episode, -1], precoder_zf[episode, -1]) > best_ave:\n","        best_ave = min(precoder_MRT[episode, -1], precoder_zf[episode, -1])\n","        agent.save_models(name='best_' + run_num)\n","\n","    print('episode', episode, 'score MRT %.1f' % precoder_MRT[episode, -1], 'score ZF %.1f' % precoder_zf[episode, -1], 'avg percentage %.1f' % best_ave)\n","\n","# Save final model\n","agent.save_models(name='At_end_' + run_num)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.status.busy":"2023-07-11T02:22:11.097321Z","iopub.status.idle":"2023-07-11T02:22:11.097849Z","shell.execute_reply":"2023-07-11T02:22:11.097624Z","shell.execute_reply.started":"2023-07-11T02:22:11.097603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The value of Alpha at end =  6.6e-06\n","The value of  Beta at end =  1.34e-05\n","The value of noise at end =  5.9999999999999995e-05\n"]}],"source":["print(\"The value of Alpha at end = \",agent.alpha)\n","print(\"The value of  Beta at end = \",agent.beta)\n","print(\"The value of noise at end = \",agent.noise)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"91536ba5-6450-4240-bdfa-d69a9146eda0","_uuid":"9741d99a-6166-45bb-b06a-c7d7c24e99c5","id":"IwHdfFddaHPn","trusted":true},"source":["# Load the agent"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9f9d6c5b-139e-4855-b129-8cf57c0debca","_uuid":"712ce7c9-321f-488f-bb52-01495b47f146","id":"vBUAd1zGpUdQ","trusted":true},"source":["### If in folder"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"c996032b-a966-41b6-979f-e3b3c80759ef","_uuid":"c076d483-41db-43f6-8bd6-57a251a492ec","collapsed":false,"execution":{"iopub.status.busy":"2023-07-11T02:22:11.100071Z","iopub.status.idle":"2023-07-11T02:22:11.100574Z","shell.execute_reply":"2023-07-11T02:22:11.100376Z","shell.execute_reply.started":"2023-07-11T02:22:11.100355Z"},"id":"OMU6LSvuhgHn","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["____Loading model of (general_snr) version best_3____\n"]}],"source":["agent.load_models(name='best_'+run_num)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0aefae70-91f6-4078-9edb-f4e781e9ebe1","_uuid":"ba44f5e2-f18b-4500-a83d-73f0eaa4cc66","id":"GA_l-DL2voZX","pycharm":{"name":"#%% md\n"},"trusted":true},"source":["# Evaluate the agent"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9a125575-4627-46c3-8b01-a7b3362c0546","_uuid":"629f46af-09b2-4f7e-a455-0608586423de","id":"8DkEcpL5FtqT","trusted":true},"source":["## Graphs"]},{"cell_type":"code","execution_count":25,"metadata":{"_cell_guid":"59374f8e-767a-4f2f-b4e7-b57ae7126634","_uuid":"a8be6f34-0e8f-4ab2-9785-633f57bec411","collapsed":false,"execution":{"iopub.status.busy":"2023-07-11T02:22:11.104639Z","iopub.status.idle":"2023-07-11T02:22:11.105106Z","shell.execute_reply":"2023-07-11T02:22:11.104919Z","shell.execute_reply.started":"2023-07-11T02:22:11.104895Z"},"id":"ZHE4yrswFwLu","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Set evaluate flag to True to skip plotting\n","evaluate = True\n","\n","if not evaluate:\n","    # Title and filename for the first plot\n","    title1 = \"Percentage\"\n","    figure1_name = f'Percentage_{run_num}.png'\n","    \n","    # Plot the learning curve for MRT and ZF percentages\n","    plot_learning_curve(\n","        y_=[precoder_MRT[:, -1], precoder_zf[:, -1]],\n","        lim=n_episode,\n","        label=[\"Percentage MRT\", \"Percentage ZF\"],\n","        figure_file=figure_file + figure1_name,\n","        title=title1\n","    )\n","\n","    # Calculate the average rewards for MRT, ZF, and the model\n","    average_mrt = np.matmul(ave_reward_mat_mrt[:, -1], weight_rate)\n","    average_zf = np.matmul(ave_reward_mat_zf[:, -1], weight_rate)\n","    average_model = np.matmul(reward_mat[:, -1], weight_rate)\n","    \n","    # Title and filename for the second plot\n","    title2 = \"Average sum reward over episodes\"\n","    figure2_name = f'Average sum reward_{run_num}.png'\n","    \n","    # Plot the learning curve for average rewards\n","    plot_learning_curve(\n","        y_=[average_model, average_mrt, average_zf],\n","        lim=n_episode,\n","        label=[\"Actor\", \"MRT\", \"ZF\"],\n","        figure_file=figure_file + figure2_name,\n","        title=title2\n","    )\n","    \n","    # Title and filename for the third plot\n","    figure3_name = f'AverageOver_100_runNumIs_{run_num}.png'\n","    \n","    # Plot the learning curve for the score history\n","    plot_learning_curve(\n","        y_=[score_history],\n","        lim=n_episode,\n","        label=[\"a\"],\n","        figure_file=figure_file + figure3_name\n","    )"]},{"cell_type":"markdown","metadata":{"_cell_guid":"cc783749-a025-4d57-8967-c3852d332957","_uuid":"2c70cc51-5b9d-4fe9-b0e7-3ca3ee1b6910","id":"poZUk-2XF7Y1","trusted":true},"source":["## Numerical Evaluation\n","\n","Calculate the reward for a large number of instances using the vectorized function and take the average.\n","\n","1. **Generate or Define Instances**: Create a large dataset of instances (channel states) for which the reward needs to be calculated.\n","2. **Calculate the Precoder Matrix Using Actor**: Use the large dataset of instances to calculate the precoder matrix.\n","3. **Calculate Rewards**: Use the vectorized reward function to compute rewards for all instances at once.\n","4. **Average the Rewards**: Calculate the average reward across all instances."]},{"cell_type":"markdown","metadata":{},"source":["### High SNR"]},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"e0c834d9-ec87-4fe6-aae5-9cac151baa05","_uuid":"898357f6-1211-45ad-8df3-dbf8ce59a25c","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:13:34.938290Z","iopub.status.busy":"2023-07-11T18:13:34.937871Z","iopub.status.idle":"2023-07-11T18:13:35.014242Z","shell.execute_reply":"2023-07-11T18:13:35.013342Z","shell.execute_reply.started":"2023-07-11T18:13:34.938257Z"},"id":"Q8Ee8-7kL5wR","jupyter":{"outputs_hidden":false},"outputId":"a4895fcb-16de-404f-934a-d4f5e7837429","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average reward by actor =\t [1.8077662  2.5782685  0.32928637 0.13635688]\n","Average reward by MRT   =\t [1.4897913 1.4548547 1.4428203 1.4338859]\n","Average reward by ZF    =\t [1.9929861 1.9929857 1.9929857 1.9929857]\n","Average reward by MMSE   =\t [2.160452  2.1545606 2.1523845 2.1511772]\n","Percentage subject to MRT =\t 83.34280369661167 %\n","Percentage subject to ZF  =\t 60.85941835143447 %\n","Percentage subject to MMSE =\t 56.29328345079189 %\n"]}],"source":["snr_user = 5.0  # Standard deviation of channel noise\n","n_time_steps_eval = 1000\n","\n","# Generate a channel matrix without the PAE effect\n","H_without_pae = channel_matrix(n_time_steps_eval, n_tx=n_tx, random=True)\n","# H_without_pae = channel_matrix(n_time_steps_eval, n_tx=n_tx, seed_number=400)\n","\n","# Apply the PAE effect to the channel matrix\n","H = PAE_3d(H_without_pae)\n","\n","# Compute the precoders for MRT, ZF, and MMSE\n","W_mrt = MRT(H)\n","W_zf = zf_mat(H)\n","W_mmse = mmse_mat(H, snr=snr_user)\n","\n","# Calculate the rewards for each precoder\n","reward_mrt = reward_for_3d(H, W_mrt, snr=snr_user)\n","reward_zf = reward_for_3d(H, W_zf, snr=snr_user)\n","reward_mmse = reward_for_3d(H, W_mmse, snr=snr_user)\n","\n","# Choose action using the agent based on the current state and evaluate\n","action_mat = agent.choose_action(state_3d_snr(H, snr_user), evaluate=True)\n","action_mat = tf.reshape(action_mat, (n_time_steps_eval, n_users, -1))\n","\n","# Calculate the evaluation rewards based on the chosen action\n","eva_reward_mat = reward_for_3d(H, tf.reshape(action_mat, (H.shape[0], n_users, -1)), snr=snr_user)\n","\n","# Compute the mean rewards for MRT, ZF, MMSE, and the model's actions\n","rmrt = np.mean(reward_mrt, axis=0)\n","rzf = np.mean(reward_zf, axis=0)\n","rmmse = np.mean(reward_mmse, axis=0)\n","rmod = np.mean(eva_reward_mat, axis=0)\n","\n","# Print the average rewards and performance metrics\n","print(f\"Average reward by actor =\\t {rmod}\")\n","print(f\"Average reward by MRT   =\\t {rmrt}\")\n","print(f\"Average reward by ZF    =\\t {rzf}\")\n","print(f\"Average reward by MMSE   =\\t {rmmse}\")\n","print(f\"Percentage subject to MRT =\\t {np.sum(rmod) * 100.0 / np.sum(rmrt)} %\")\n","print(f\"Percentage subject to ZF  =\\t {np.sum(rmod) * 100.0 / np.sum(rzf)} %\")\n","print(f\"Percentage subject to MMSE =\\t {np.sum(rmod) * 100.0 / np.sum(rmmse)} %\")"]},{"cell_type":"markdown","metadata":{},"source":["### Low SNR"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T18:14:09.858009Z","iopub.status.busy":"2023-07-11T18:14:09.857646Z","iopub.status.idle":"2023-07-11T18:14:09.923906Z","shell.execute_reply":"2023-07-11T18:14:09.921667Z","shell.execute_reply.started":"2023-07-11T18:14:09.857981Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average reward by actor =\t [0.02731172 0.04411576 0.00358495 0.00134194]\n","Average reward by MRT   =\t [0.02339773 0.02429889 0.02290916 0.02424937]\n","Average reward by ZF    =\t [0.00890983 0.00890983 0.00890983 0.00890983]\n","Average reward by MMSE   =\t [0.02320887 0.02402772 0.02272518 0.02398323]\n","Percentage subject to MRT =\t 80.49573792598257 %\n","Percentage subject to ZF  =\t 214.2418306528997 %\n","Percentage subject to MMSE =\t 81.27559464271948 %\n"]}],"source":["# Set the standard deviation of channel noise\n","snr_user = 0.01  \n","n_time_steps_eval = 1000\n","\n","# Generate a channel matrix without the PAE effect\n","H_without_pae = channel_matrix(n_time_steps_eval, n_tx=n_tx, random=True)\n","# H_without_pae = channel_matrix(n_time_steps_eval, n_tx=n_tx, seed_number=400)\n","\n","# Apply the PAE effect to the channel matrix\n","H = PAE_3d(H_without_pae)\n","\n","# Compute the precoders for MRT, ZF, and MMSE\n","W_mrt = MRT(H)\n","W_zf = zf_mat(H)\n","W_mmse = mmse_mat(H, snr=snr_user)\n","\n","# Calculate the rewards for each precoder based on the channel matrix\n","reward_mrt = reward_for_3d(H, W_mrt, snr=snr_user)\n","reward_zf = reward_for_3d(H, W_zf, snr=snr_user)\n","reward_mmse = reward_for_3d(H, W_mmse, snr=snr_user)\n","\n","# Choose action using the agent based on the current state and evaluate\n","action_mat = agent.choose_action(state_3d_snr(H, snr_user), evaluate=True)\n","\n","# Reshape the action matrix for evaluation\n","action_mat = tf.reshape(action_mat, (n_time_steps_eval, n_users, -1))\n","\n","# Calculate the evaluation rewards based on the chosen action\n","eva_reward_mat = reward_for_3d(H, tf.reshape(action_mat, (H.shape[0], n_users, -1)), snr=snr_user)\n","\n","# Compute the mean rewards for MRT, ZF, MMSE, and the model's actions\n","rmrt = np.mean(reward_mrt, axis=0)\n","rzf = np.mean(reward_zf, axis=0)\n","rmmse = np.mean(reward_mmse, axis=0)\n","rmod = np.mean(eva_reward_mat, axis=0)\n","\n","# Print the average rewards and performance metrics\n","print(f\"Average reward by actor =\\t {rmod}\")\n","print(f\"Average reward by MRT   =\\t {rmrt}\")\n","print(f\"Average reward by ZF    =\\t {rzf}\")\n","print(f\"Average reward by MMSE   =\\t {rmmse}\")\n","print(f\"Percentage subject to MRT =\\t {np.sum(rmod) * 100.0 / np.sum(rmrt)} %\")\n","print(f\"Percentage subject to ZF  =\\t {np.sum(rmod) * 100.0 / np.sum(rzf)} %\")\n","print(f\"Percentage subject to MMSE =\\t {np.sum(rmod) * 100.0 / np.sum(rmmse)} %\")"]},{"cell_type":"markdown","metadata":{},"source":["### Single Instance (State) Evaluation"]},{"cell_type":"code","execution_count":49,"metadata":{"_cell_guid":"6a788d53-bf9d-4f81-88de-db2f69e598a8","_uuid":"5fde2a48-3192-4136-9cda-67d8c3d88f8d","collapsed":false,"execution":{"iopub.execute_input":"2023-07-11T18:13:24.456818Z","iopub.status.busy":"2023-07-11T18:13:24.456423Z","iopub.status.idle":"2023-07-11T18:13:24.478464Z","shell.execute_reply":"2023-07-11T18:13:24.477076Z","shell.execute_reply.started":"2023-07-11T18:13:24.456787Z"},"id":"tOyD7g5VRjsD","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reward by actor =\t [0.01973256 0.08081265 0.00160834 0.0007194 ]\n","Reward by MRT   =\t [0.01952235 0.05102535 0.00830825 0.0231408 ]\n","Reward by ZF    =\t [0.01045827 0.01045827 0.01045827 0.01045827]\n","Reward by MMSE  =\t [0.01603819 0.01450478 0.01131923 0.01225454]\n","Percentage subject to MRT  =\t 100.85905746706793 %\n","Percentage subject to ZF   =\t 245.91281863543722 %\n","Percentage subject to MMSE  =\t 190.09449603434413 %\n"]}],"source":["# Generate channel states using the PAE effect with a fixed seed for reproducibility\n","h = PAE(channel_states(n_tx=n_tx, seed_number=42, random=True))\n","\n","# Reshape the channel states to 3D for processing\n","h_3d = np.expand_dims(h.reshape((n_users, n_tx * 2)), axis=0)\n","\n","# Calculate rewards using different precoding techniques\n","reward_mrt_sample = reward_for_3d(h_3d, MRT(h_3d), snr=snr_user)[0]\n","reward_zf_sample = reward_for_3d(h_3d, zf_mat(h_3d), snr=snr_user)[0]\n","reward_mmse_sample = reward_for_3d(h_3d, mmse_mat(h_3d), snr=snr_user)[0]\n","\n","# Choose action using the agent based on the current state and evaluate\n","precoder = agent.choose_action(state_plus_snr(h, snr_user), evaluate=True)\n","\n","# Calculate the reward for the chosen action\n","reward_model_sample = get_reward_vectorize(h, precoder, snr=snr_user)\n","\n","# Print the rewards obtained from the actor and different methods\n","print(f\"Reward by actor =\\t {reward_model_sample}\")\n","print(f\"Reward by MRT   =\\t {reward_mrt_sample}\")\n","print(f\"Reward by ZF    =\\t {reward_zf_sample}\")\n","print(f\"Reward by MMSE  =\\t {reward_mmse_sample}\")\n","\n","# Calculate and print the percentage performance relative to each method\n","print(f\"Percentage subject to MRT  =\\t {np.sum(reward_model_sample) * 100.0 / np.sum(reward_mrt_sample)} %\")\n","print(f\"Percentage subject to ZF   =\\t {np.sum(reward_model_sample) * 100.0 / np.sum(reward_zf_sample)} %\")\n","print(f\"Percentage subject to MMSE  =\\t {np.sum(reward_model_sample) * 100.0 / np.sum(reward_mmse_sample)} %\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3498260,"sourceId":6106608,"sourceType":"datasetVersion"},{"datasetId":3504405,"sourceId":6114834,"sourceType":"datasetVersion"},{"sourceId":135953607,"sourceType":"kernelVersion"}],"dockerImageVersionId":30513,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":4}
